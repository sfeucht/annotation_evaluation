# This is the most recent file produced

import csv
import re
import os
from scipy.stats import pearsonr
import time

# count the number of documents assigned to annotators
annotators = {"Sheridan":[],"Muskaan":[],"Kate":[]}
with open('annotation_info.csv','r') as f:
    reader = csv.reader(f)
    for idx,row in enumerate(reader):
        if idx != 0 and len(row) > 0:
            index = re.sub('[^0-9]','',row[0])
            for key in annotators:
                if key.lower() in [i.strip() for i in row[1].split(",")]:
                    annotators[key].append(int(index))
print(annotators)
for annotator in annotators:
    print(len(annotators[annotator]))

# for each document, record whether it was generated by Grover or humans
generation = {"human":[],"grover":[]}
with open('info.csv','r') as f:
    reader = csv.reader(f)
    for idx,row in enumerate(reader):
        if idx != 0 and "COLLECTED" not in row:
            if "original" in row[3]:
                generation["human"].append(int(row[0].strip()))
            elif "grover" in row[3]:
                generation["grover"].append(int(row[0].strip()))
# print the count
print(len(generation["human"]))
print(len(generation["grover"]))

# counter = 0
# file_counter = 0
# for annotator in annotators:
#     files = os.listdir("{}/".format(annotator))
#     if len(files) != 0:
#         for file in files:
#             if "annotation" not in file and "practice" not in file:
#                 file_counter += 1
#                 with open("{}/{}".format(annotator,file),"r") as f:
#                     for line in f:
#                         if line.strip() != "" and "***" not in line:
#                             counter += 1
#
# avg_len = counter / file_counter
avg_len = 67.3157894736842 # avg length calculated based on the first 40
# annotated docs and then kept constant to ensure comparability
print(avg_len)

# count the number of average lengths of documents that has been annotated by
# each person
annotated_counter = {"Sheridan":0,"Muskaan":0,"Kate":0}
for annotator in annotators:
    counter = 0
    files = os.listdir("{}/".format(annotator))
    if len(files) != 0:
        for file in files:
            if "annotation" not in file and "practice" not in file:
                with open("{}/{}".format(annotator,file),"r") as f:
                    for line in f:
                        if line.strip() != "" and "***" not in line:
                            counter += 1
                            if counter == int(avg_len):
                                annotated_counter[annotator] += 1
                                counter = 0
print(annotated_counter)

# simple count of causal relations, elaborations and faulty coherence relations
# in human and Grover documents, respectively.
# NOTE: Confidence ratings are ignored below.

ce_counter = {"human":0,"grover":0}
x_counter = {"human":0,"grover":0}
elab_counter = {"human":0, "grover":0}
counter = 0
ce_idx_counter = {}

for annotator in annotators:
    files = os.listdir("{}/".format(annotator))
    if len(files) != 0:
        for file in files:
            if "annotation" in file:
                with open("{}/{}".format(annotator,file),"r") as f:
                    for line in f:
                        if line.strip() != "":
                            counter += 1
                            split_line = line.strip().split("//")[0].split()
                            index = int(re.sub('[^0-9]','',file))
                            ce_idx_counter[index] = 0
                            if "ce" in line:
                                ce_idx_counter[index] += 1
                                if index in generation["human"]:
                                    ce_counter["human"] += 1
                                elif index in generation["grover"]:
                                    ce_counter["grover"] += 1
                                else:
                                    print(index)
                                    raise Exception("Index could not be found")
                            elif "elab" in line:
                                if index in generation["human"]:
                                    elab_counter["human"] += 1
                                elif index in generation["grover"]:
                                    elab_counter["grover"] += 1
                                else:
                                    print(index)
                                    raise Exception("Index could not be found")
                            if ("x" in line and "examp" not in line) or "deg" in line or "rep" in line:
                                if index in generation["human"]:
                                    x_counter["human"] += 1
                                elif index in generation["grover"]:
                                    x_counter["grover"] += 1
                                else:
                                    print(index)
                                    raise Exception("Index could not be found")

# Calculate the accuracy of annotators in determining the author of a document
# At the same time, extract narrativity ratings from each annotated document to
# correlate with the number of causal relations
confusion = {"tp":0,"fp":0,"tn":0,"fn":0}
narrativity = {}
for annotator in annotators:
    files = os.listdir("{}/".format(annotator))
    if len(files) != 0:
        for file in files:
            if "annotation" not in file and "practice" not in file:
                with open("{}/{}".format(annotator,file),"r") as f:
                    index = int(re.sub('[^0-9]','',file))
                    for line in f:
                        if "***" in line:
                            if "HUMAN" in line and index in generation["human"]:
                                confusion["tp"] += 1
                            elif "HUMAN" in line and index in generation["grover"]:
                                confusion["fp"] += 1
                            elif "ALGORITHM" in line and index in generation["grover"]:
                                confusion["tn"] += 1
                            elif "ALGORITHM" in line and index in generation["human"]:
                                confusion["fn"] += 1

                            just_ratings = line.replace("***","")
                            just_ratings = just_ratings.strip().split()

                            # if just_ratings[3] != "0":
                            narrativity[index] = int(just_ratings[3])

# calculate Pearson correlation between narrativity and causal relation count
ce = []
narr = []
for element in narrativity:
    ce.append(ce_idx_counter[element])
    narr.append(narrativity[element])

print(narr)
print(ce)
print(pearsonr(ce,narr))

# save the weekly count of annotated documents to a text file
with open("weekly counts.txt","a+") as f:
    print(time.strftime('%m/%d/%Y'),end="\n",file=f)
    print(annotated_counter,end="\n",file=f)

# print out causal relation, elaboration and faulty coherence relation counters
print(ce_counter)
print(elab_counter)
print(x_counter)
print(counter)
print(confusion)


# TODO: coherence relation lengths and direction
